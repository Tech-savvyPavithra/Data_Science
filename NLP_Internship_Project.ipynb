{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW9VSgeB7EhFbRcel9C8QN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tech-savvyPavithra/Data_Science/blob/main/NLP_Internship_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJkx8mmn3BEk",
        "outputId": "a2b5fc09-2c4f-4e92-dc4c-36441f241165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "9Rwoab5L3gK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCxy71qE3-nx",
        "outputId": "b84f7ce8-58df-42af-f7b4-56eee2dba8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize"
      ],
      "metadata": {
        "id": "gnvqShlb3_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenizer"
      ],
      "metadata": {
        "id": "k2ge7HJOn4bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text=\"This is Pavithra. This is Drishya. This is Sujatha.\"\n",
        "# Tokenize the sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(\"No.of tokens sentences:\",len(sentences))"
      ],
      "metadata": {
        "id": "KA2NKkBA3gYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfa9d0a-14c7-421b-d036-c33a3469bd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is Pavithra.', 'This is Drishya.', 'This is Sujatha.']\n",
            "No.of tokens sentences: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenizer"
      ],
      "metadata": {
        "id": "AL-Cg8I3oKUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"i feel hungry now! I want a pizza.\"\n",
        "x=print(word_tokenize(text))\n",
        "print(\"No.of token_words:\",len(word_tokenize(text)))"
      ],
      "metadata": {
        "id": "45MMgX1d3gz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b5dbaf-e37a-4507-e8ba-65df4d59f9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'feel', 'hungry', 'now', '!', 'I', 'want', 'a', 'pizza', '.']\n",
            "No.of token_words: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text=\"Happy Morning!.All the best.\"\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "fQG_1jK33hns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72560f22-ce2f-45a0-a0f3-f90fef58edbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy', 'Morning', '!', '.All', 'the', 'best', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "_C7CnJ1lpODJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text=\"Happy Morning!.All the best.\"\n",
        "lr=text.lower()\n",
        "ur=text.upper()\n",
        "print(ur)\n",
        "print(lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_FPNipGpTN2",
        "outputId": "a7df6123-2364-4ca4-b8a8-b32e4a9dc57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HAPPY MORNING!.ALL THE BEST.\n",
            "happy morning!.all the best.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stemmer = nltk.PorterStemmer()\n",
        "words = [\"looking\", \"look\", \"looker\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Uv9cjmpm7x",
        "outputId": "bcecab23-9a3a-4262-9b02-4d6da817803b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['look', 'look', 'looker']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps=PorterStemmer()\n",
        "words=['mystery','mysterical','Biology','union','unity','hello']\n",
        "for i in words:\n",
        "  print(\"Wordsstem:\",ps.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF2M8mCwqFcJ",
        "outputId": "8e8f5d06-6c06-40a5-9968-59504131bb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wordsstem: mysteri\n",
            "Wordsstem: myster\n",
            "Wordsstem: biolog\n",
            "Wordsstem: union\n",
            "Wordsstem: uniti\n",
            "Wordsstem: hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "U0Fp4u_eqyjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Eno1H_qrdS",
        "outputId": "46ac0887-541d-43f5-9881-04c9deac8f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNet Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemma = WordNetLemmatizer()\n",
        "text = \"Make hay , while the sun shines.\"\n",
        "words = word_tokenize(text)\n",
        "for w in words:\n",
        "    print(w, \" : \", lemma.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWgLpAyXq8r6",
        "outputId": "3fb5450a-536a-4f20-b4b1-f51ac9e86d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make  :  Make\n",
            "hay  :  hay\n",
            ",  :  ,\n",
            "while  :  while\n",
            "the  :  the\n",
            "sun  :  sun\n",
            "shines  :  shine\n",
            ".  :  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemma = lemmatizer.lemmatize(\"speaking\", pos=\"v\")\n",
        "lemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Eyaq4rgEr3pA",
        "outputId": "cf0d0ea5-eef6-4235-c2ef-427cea3a112f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'speak'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords"
      ],
      "metadata": {
        "id": "PUkTyz4gsP8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhOJr9gtsSi5",
        "outputId": "999bbaf7-2d4a-49f2-a672-b26dee64e31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I21nIGAHsZgI",
        "outputId": "5590e609-5a35-4a7f-9b20-713e8dbce4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"you've\", 'there', \"it's\", \"that'll\", 'be', 'most', \"doesn't\", 'further', 'have', 'just', 'theirs', 'through', 'its', 'll', 'same', \"shan't\", 'did', 'you', 'am', 'out', 'shouldn', 's', 'nor', 'when', 'can', 'mustn', 'not', 'any', \"shouldn't\", \"couldn't\", 'my', 'because', 'more', 'by', 'those', 'so', 'whom', 'hers', 'your', 'down', 'between', 'been', \"she's\", 'yourself', 'all', 'i', 'during', 'mightn', 'an', 'until', 'was', 'into', 'which', 'as', \"aren't\", 'haven', 'needn', 'he', 'were', 'wouldn', 'than', 'in', \"should've\", 'ours', 'too', 'for', 'do', \"you'd\", 'before', 'aren', 're', 'does', 'only', 'didn', 'who', 'some', 'such', 'the', 'then', 'but', \"needn't\", 'his', 'her', 'each', 'we', 'up', 'from', 'no', \"wouldn't\", 'had', 'a', 'these', 'isn', 'ma', 'now', 'of', 'being', 't', 'again', 'him', 'yours', 'having', 'above', 'this', 'shan', 'has', 'don', 'is', 'after', \"isn't\", 'why', 'about', 'to', 'very', 'hadn', 'while', 'what', 'on', 'ourselves', 'under', 'd', 'me', 'if', 'their', 'at', 'own', 'with', \"hasn't\", \"you're\", 've', 'y', 'them', 'wasn', \"won't\", \"mustn't\", 'once', 'doing', 'doesn', 'our', 'over', 'myself', 'itself', 'himself', 'herself', 'and', 'how', \"wasn't\", 'yourselves', 'other', \"didn't\", \"weren't\", 'against', 'weren', \"hadn't\", \"don't\", 'here', 'ain', 'are', 'won', 'that', 'she', 'they', 'themselves', 'both', 'm', 'few', \"mightn't\", \"you'll\", 'will', 'where', 'o', 'should', 'below', 'hasn', \"haven't\", 'or', 'it', 'off', 'couldn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords removal\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"i need to take a break from social media.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(text)\n",
        "filtered_sentence = []\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3zN-3W3se-M",
        "outputId": "acf45aa1-11ab-4f8d-b592-a582714c55c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'need', 'to', 'take', 'a', 'break', 'from', 'social', 'media', '.']\n",
            "['need', 'take', 'break', 'social', 'media', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pos Tag"
      ],
      "metadata": {
        "id": "PkyXPNRYtBED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2bBWmt6s7P4",
        "outputId": "65ba26d2-fa28-4a5c-ff2d-edaee97a5883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part-of-Speech tagging\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "text=\"i am thirsty, kindly fetch me some water.\"\n",
        "sent = nltk.word_tokenize(text)\n",
        "print(sent)\n",
        "postag = nltk.pos_tag(sent)\n",
        "print(postag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7TQhmUUtJIF",
        "outputId": "20f760bd-8b49-4e16-a6d7-f1c877533e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'thirsty', ',', 'kindly', 'fetch', 'me', 'some', 'water', '.']\n",
            "[('i', 'NN'), ('am', 'VBP'), ('thirsty', 'NN'), (',', ','), ('kindly', 'RB'), ('fetch', 'VB'), ('me', 'PRP'), ('some', 'DT'), ('water', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER Tags"
      ],
      "metadata": {
        "id": "nwsza28RtndP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j2NXWMntrmG",
        "outputId": "4725b9ff-ca02-4038-8150-e60dddb82fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mv8he9stwSn",
        "outputId": "03b68460-27ec-4379-e378-c8d6ed89b369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Hi Guys. Priya went to France. Priya bought a Maruti car.\"\n",
        "token = word_tokenize(example)\n",
        "postag = nltk.pos_tag(token)\n",
        "ner = nltk.ne_chunk(postag, binary= False)\n",
        "print(ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpB6qu02txlK",
        "outputId": "a039a012-43ca-45a9-9bbf-33db88820304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Hi/NNP\n",
            "  Guys/NNP\n",
            "  ./.\n",
            "  (PERSON Priya/NNP)\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE France/NNP)\n",
            "  ./.\n",
            "  (PERSON Priya/NNP)\n",
            "  bought/VBD\n",
            "  a/DT\n",
            "  (GPE Maruti/NNP)\n",
            "  car/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "s2 = 'She looks beautiful!.He looks Ugly!'\n",
        "print(\"polarity score for s2:\")\n",
        "sia.polarity_scores(s2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASlS54IauTYG",
        "outputId": "7db789dc-98a8-4ca5-d9f7-12719459cf58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polarity score for s2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.493, 'neu': 0.507, 'pos': 0.0, 'compound': -0.5972}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}